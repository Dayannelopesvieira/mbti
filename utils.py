#import libraries
import numpy as np
import pandas as pd
import matplotlib.pylab as plt
import seaborn as sns#Understanding my variables
import re

import pandas as pd
import pickle
from scipy import sparse
import numpy as np
import scipy
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import warnings

from sklearn.base import BaseEstimator, TransformerMixin
import pandas as pd
from sklearn.feature_extraction import text as text_extraction
import pandas as pd
import nltk
import re
from nltk.tokenize import word_tokenize, wordpunct_tokenize
from nltk.stem import SnowballStemmer
from nltk.stem import WordNetLemmatizer
import nltk
import sys
import traceback
from sklearn.cluster import MiniBatchKMeans

import string
import re
import unicodedata
import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer

from sklearn.model_selection import train_test_split

warnings.filterwarnings('ignore')
nltk.download('wordnet')

#some functions 
def find_optimal_clusters(data, max_k):
    """
   the approach chose was to plot the SSE- for a range of cluster sizes. 
   get the elbow where the SSE begins to level off.
   
   batch and init sizes higher to ignore the noises. 
   random states to start always with some random data point as centroid
   
    """
    iters = range(2, max_k+1, 2)
    
    sse = []
    for k in iters:
        sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20).fit(data).inertia_)
        print('Fit {} clusters'.format(k))
        
    f, ax = plt.subplots(1, 1)
    ax.plot(iters, sse, marker='o')
    ax.set_xlabel('Cluster Centers')
    ax.set_xticks(iters)
    ax.set_xticklabels(iters)
    ax.set_ylabel('SSE')
    ax.set_title('SSE by Cluster Center Plot')

def get_top_keywords(data, clusters, labels, n_terms):
    """
        it gets the top keywords besed on tfidf score
        
        it computes the average value across all dimensions (using pandas) grouped by the cluster label. 
        next it find the top words sorting the average values for each row, and taking the top N.
    """
    df = pd.DataFrame(data.todense()).groupby(clusters).mean()
    data_name_complete=pd.DataFrame()
    for i,r in df.iterrows():
        data_name=pd.DataFrame()
        data_name['clusters']=[i]
        data_name['keywords']=  [','.join([labels[t] for t in np.argsort(r)[-n_terms:]])]
        data_name_complete=data_name_complete.append(data_name)
    return data_name_complete

def get_clusters_kmeans(X,number_k):
    clusters = MiniBatchKMeans(n_clusters=number_k, 
                            init_size=1024, 
                            batch_size=2048, 
                            random_state=20).fit_predict(X)
    return clusters


import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from sklearn.decomposition import PCA

def plot_pca(data, labels):
    """
    plot the clusters generated by the KMeans operation. 
    1st plot uses PCA to see better the global structure of the data. 
    
    """
    max_label = max(labels)
    max_items = np.random.choice(range(data.shape[0]),size=data.shape[0], replace=False)
    
    pca = PCA(n_components=0.95).fit_transform(data[max_items,:].todense())
    
    
    idx = np.random.choice(range(pca.shape[0]), size=pca.shape[0], replace=False)
    label_subset = labels[max_items]
    label_subset = [cm.hsv(i/max_label) for i in label_subset[idx]]
    
    f, ax = plt.subplots(1, 1, figsize=(10, 6))
    
    ax.scatter(pca[idx, 0], pca[idx, 1], c=label_subset)
    ax.set_title('PCA Cluster Plot')

def bar_split_text_per_clusters(clusters,data,column):   

    #plot the bar : quantity of tickets per clusters 
    data['clusters']=clusters
    data['clusters']=data['clusters'].astype(int)
    grouped = data.groupby('clusters')
    grouped=grouped[column].agg(np.size).reset_index()
    grouped=grouped.rename(columns={column:'count'}).drop_duplicates()
    plt.figure(figsize=(8,5))
    plt.ylabel('Quantity of tickets')
    plt.xlabel('Name/Number of the cluster')
    plt.title('Quantity of tickets per cluster')
    plt.xticks(range(len(grouped)))
    plt.bar(grouped['clusters'],grouped['count'])
    plt.show()